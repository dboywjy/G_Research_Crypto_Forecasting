{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import talib \n",
    "from talib import abstract\n",
    "from joblib import delayed,Parallel,cpu_count\n",
    "cpu_nums=cpu_count()\n",
    "slice_windows=30"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data=pd.read_csv('train.csv')\n",
    "asset1=train_data[train_data['Asset_ID']==1]\n",
    "asset1.to_csv('asset1.csv',index=False)\n",
    "del train_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data=pd.read_csv('asset1.csv')\n",
    "train_data['amount']=train_data['Count']*train_data['VWAP']\n",
    "train_data=train_data.rename(columns={'Close':'close','Open':'open','High':'high','Low':'low','Volume':'volume'  })\n",
    "\n",
    "ta_factors=['HT_DCPERIOD','HT_DCPHASE','HT_PHASOR','HT_SINE','HT_TRENDMODE','ADD',\n",
    "    'DIV','APO','BOP','CCI','CMO','MACD','MFI','MOM','PPO','ROC','RSI','TRIX','WILLR',\n",
    "    'BBANDS','DEMA','EMA','HT_TRENDLINE','KAMA','MA','MAMA','MIDPOINT','MIDPRICE','SAR',\n",
    "    'SAREXT','SMA','T3','TEMA','TRIMA','WMA','AVGPRICE','MEDPRICE','TYPPRICE','WCLPRICE',\n",
    "    'LINEARREG','LINEARREG_ANGLE','LINEARREG_INTERCEPT','LINEARREG_SLOPE','STDDEV','TSF',\n",
    "    'TRANGE','AD','ADOSC','OBV']\n",
    "\n",
    "def generate_talib_factor(df,ta_factor):\n",
    "    \"\"\"\"\n",
    "    调取talib的因子，按照默认参数进行求值\n",
    "    \"\"\"\n",
    "    target_fun=abstract.Function(ta_factor)\n",
    "    ta_values=target_fun(df)\n",
    "    #if-else below to cope with different result that some talib functions will return multi-columns while others only one\n",
    "    #eg. MAXMIN return MAX and MIN two values\n",
    "    if len(ta_values.shape)==1:\n",
    "        ta_values=pd.DataFrame(ta_values,columns=[f'{ta_factor}'])\n",
    "    else:\n",
    "        ta_values=pd.DataFrame(ta_values)\n",
    "        colname=ta_values.columns\n",
    "        new_colname={}\n",
    "        for col in colname:\n",
    "            new_colname.update( {col:f'{col}'})\n",
    "        ta_values.rename(columns=new_colname,inplace=True)\n",
    "    return ta_values\n",
    "\n",
    "#pd.concat(Parallel(cpu_nums)(delayed(generate_talib_factor)(train_data,factor) for factor in ta_factors),axis=1)\n",
    "panel=pd.concat( [generate_talib_factor(train_data,factor) for factor in ta_factors],axis=1)\n",
    "panel=(panel-panel.rolling(slice_windows).mean())/panel.rolling(slice_windows).std()\n",
    "panel=pd.concat([panel,train_data[['Target','Asset_ID']]],axis=1)#the last two col is target and aseet_ID"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import pytorch_lightning as pl\n",
    "import torch.multiprocessing\n",
    "from joblib import Parallel, delayed\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "\n",
    "class GruDataset(Dataset):\n",
    "    \"\"\"\n",
    "        Dataset for training\n",
    "    \"\"\"\n",
    "    def __init__(self,dataset, seq_len,train=True):\n",
    "        #Train denote whether this dataset is used for training(testing otherwise)\n",
    "        #if train，up and dwon threshold filter will work; if test, no such\n",
    "        def judge(df,up_threshold=0.2,down_threshold=0.0005):\n",
    "            #20% abs change in target \n",
    "            length=df.shape[0]-seq_len+1\n",
    "            df=df.copy()\n",
    "            #float32 to save memory\n",
    "            df=df.values.astype(np.float32)\n",
    "            x_list=[]\n",
    "            y_list=[]\n",
    "            for idx in tqdm(range(0,length)):\n",
    "                #像此处的index的部分，取决于\n",
    "                x=df[idx:idx+seq_len,:-2]\n",
    "                y=df[idx+seq_len-1,-2]\n",
    "                #don't study severe price change\n",
    "                if train:\n",
    "                    if (abs(y)>down_threshold) and (abs(y)<up_threshold):\n",
    "                        x_list.append(x)\n",
    "                        y_list.append(y)\n",
    "                else:\n",
    "                        x_list.append(x)\n",
    "                        y_list.append(y)\n",
    "            length=len(x_list)\n",
    "            return [x_list,y_list,length]\n",
    "        #settle x array and y value in the list so that can get them easily\n",
    "        #final_set=Parallel(cpu_nums)(delayed(judge)(df) for name,df in dataset.groupby('Asset_ID') ) \n",
    "        self.final_set=[judge(df) for name,df in dataset.groupby('Asset_ID')]\n",
    "        self.length=sum([ self.final_set[i][2] for i in range(len( self.final_set))])\n",
    "        self.dataset=dataset\n",
    "        self._x=[ self.final_set[i][0][j] for i in range(len( self.final_set)) for j in range(len( self.final_set[i][0])) ]\n",
    "        self._y=[ self.final_set[i][1][j] for i in range(len( self.final_set)) for j in range(len( self.final_set[i][1])) ]\n",
    "    def __len__(self):\n",
    "        return self.length\n",
    "    def __getitem__(self, idx):\n",
    "        x=self._x[idx]\n",
    "        y=self._y[idx]\n",
    "        return x,y\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1956263\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1956263/1956263 [00:05<00:00, 375601.54it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X: \n",
      "\n",
      " [[ 1.3843755  -0.8948697  -0.6982844  ... -1.7272546  -0.8865081\n",
      "  -0.7968843 ]\n",
      " [ 1.2581758  -0.84471047 -0.88629806 ... -1.6411021  -0.5775061\n",
      "  -0.9466304 ]\n",
      " [ 1.2455662  -0.78142214 -0.730725   ... -1.5954736  -0.40727997\n",
      "  -1.0813993 ]\n",
      " ...\n",
      " [-0.85551566  3.0285854   2.3037703  ... -1.1901703   0.6843566\n",
      "  -0.6732313 ]\n",
      " [-1.1853576   3.1191325   1.851343   ... -1.167127    0.8078065\n",
      "  -0.48269978]\n",
      " [-1.4924889   3.2350836   1.8776377  ... -1.1309848   1.0269927\n",
      "  -0.2911204 ]] Y: \n",
      "\n",
      " -0.0020541039\n",
      "length \n",
      "\n",
      " 1286016\n"
     ]
    }
   ],
   "source": [
    "#example\n",
    "gd=GruDataset(panel,20)\n",
    "print(\"X: \\n\\n\",gd[100][0],\"Y: \\n\\n\",gd[100][1],\"\\n\\n\")\n",
    "print(\"length \\n\\n\",len(gd))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#A temo Dataset, used while cut train and validation  \n",
    "class TempDataset(Dataset):\n",
    "        def __init__(self,x,y):\n",
    "            self.x=x\n",
    "            self.y=y\n",
    "        def __len__(self):\n",
    "            return len(self.x)\n",
    "        def __getitem__(self, idx):\n",
    "            return self.x[idx],self.y[idx]\n",
    "\n",
    "def data_split(df,batch_size=512,train_portion=0.75,seq_len=2400):\n",
    "    \"\"\"spilt data in to train set and validation set, and transform \n",
    "    into Grudf in model_structure and DataLoader in pytorch\"\"\"\n",
    "    df_train_x=df[:int(len(df)*train_portion)][0]\n",
    "    df_train_y=df[:int(len(df)*train_portion)][1]\n",
    "    df_valid_x=df[int(len(df)*train_portion):][0]\n",
    "    df_valid_y=df[int(len(df)*train_portion):][1]\n",
    "    df_train=TempDataset(df_train_x,df_train_y)\n",
    "    df_valid=TempDataset(df_valid_x,df_valid_y)\n",
    "    #Dataloader\n",
    "    train_dataloader = DataLoader(df_train, batch_size=batch_size, shuffle=True,drop_last=True)\n",
    "    valid_dataloader = DataLoader(df_valid, batch_size=batch_size, shuffle=True,drop_last=True)\n",
    "    return train_dataloader,valid_dataloader\n",
    "\n",
    "\n",
    "class GRUNN(pl.LightningModule):\n",
    "    def __init__(self,config ,feature_size:int=52):\n",
    "        super().__init__()\n",
    "        self.feature_size=feature_size\n",
    "        self.lr=config['lr']\n",
    "        self.l2=config['l2']\n",
    "        self.dropout=config['dropout']\n",
    "        self.layer_norm = nn.LayerNorm(feature_size)\n",
    "        self.gru = nn.GRU(feature_size,int(feature_size*2/3), 2, batch_first=True, dropout=self.dropout)#hiddenlayer/inputlayer=2/3\n",
    "        self.linear = nn.Linear(int(feature_size*2/3), 1) #dense\n",
    "\n",
    "    def forward(self, x):\n",
    "        #x = self.layer_norm(x)#actually this step is not necessary\n",
    "        out, _ = self.gru(x)#return output,h_n\n",
    "        out = self.linear(out[:, -1])\n",
    "        return out\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        optimizer = torch.optim.Adam(self.parameters(), lr=self.lr,weight_decay=self.l2)\n",
    "        return optimizer\n",
    "\n",
    "    def training_step(self, train_batch, batch_idx):\n",
    "        x, y = train_batch\n",
    "        y_pred = self(x)\n",
    "        # weight = torch.Tensor([1., 0.072, 1.]).cuda()\n",
    "        # loss = F.cross_entropy(y_pred, y, weight=weight)\n",
    "        loss_fun = nn.MSELoss()\n",
    "        loss = loss_fun(y_pred, y)\n",
    "        # loss = F.binary_cross_entropy_with_logits(y_pred, y.view(-1, 1).to(torch.float))\n",
    "        self.log('train/loss', loss.item())\n",
    "        return loss\n",
    "\n",
    "    def predict_step(self, batch, batch_idx: int , dataloader_idx: int = None):\n",
    "        x,y=batch\n",
    "        return [self(x),y]\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
