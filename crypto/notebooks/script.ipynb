{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import talib \n",
    "from talib import abstract\n",
    "from joblib import delayed,Parallel,cpu_count\n",
    "cpu_nums=cpu_count()\n",
    "slice_windows=30"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jwangiy/crypto/notebooks/../src/utils.py:155: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  train_data['amount']=train_data['Count']*train_data['VWAP']\n",
      "/home/jwangiy/crypto/notebooks/../src/utils.py:155: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  train_data['amount']=train_data['Count']*train_data['VWAP']\n",
      "/home/jwangiy/crypto/notebooks/../src/utils.py:155: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  train_data['amount']=train_data['Count']*train_data['VWAP']\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "sys.path.append(\"../src/\")\n",
    "from utils import *\n",
    "df = pd.read_csv(\"../data/raw_data/train.csv\")\n",
    "weight = pd.read_csv(\"../data/raw_data/asset_details.csv\")\n",
    "Asset_ID = 1\n",
    "\n",
    "# kfold parameters\n",
    "cv = 3\n",
    "plength=0.8\n",
    "ptrain=0.8\n",
    "pgap=0.05\n",
    "cv_method = 'best'\n",
    "# missing data\n",
    "timestamp_fill = False\n",
    "method = 'drop'\n",
    "weight = weight\n",
    "df = df\n",
    "dft = df[df['Asset_ID'] == Asset_ID].sort_values(by=['timestamp'], ascending=True)\n",
    "# sample\n",
    "dft = dft.iloc[math.floor(dft.shape[0]/6*5):]\n",
    "cvs = CVsample(df=dft, ival='timestamp', cv=cv, plength=plength, ptrain=ptrain, pgap=pgap)\n",
    "cvs.main()\n",
    "\n",
    "# test\n",
    "test = dft[dft['timestamp'] > max(cvs.validation_ends)]\n",
    "tb = talib(test)\n",
    "test_panel = tb.main()\n",
    "\n",
    "[train, gap, validation] = cvs.get_df(2)\n",
    "# feature\n",
    "tb = talib(train)\n",
    "train_panel = tb.main()\n",
    "\n",
    "tb = talib(validation)\n",
    "val_panel = tb.main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import pytorch_lightning as pl\n",
    "import torch.multiprocessing\n",
    "from joblib import Parallel, delayed\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "\n",
    "class GruDataset(Dataset):\n",
    "    \"\"\"\n",
    "        Dataset for training\n",
    "    \"\"\"\n",
    "    def __init__(self,dataset, seq_len,train=True):\n",
    "        #Train denote whether this dataset is used for training(testing otherwise)\n",
    "        #if train，up and dwon threshold filter will work; if test, no such\n",
    "        def judge(df,up_threshold=0.2,down_threshold=0.0005):\n",
    "            #20% abs change in target \n",
    "            length=df.shape[0]-seq_len+1\n",
    "            df=df.copy()\n",
    "            #float32 to save memory\n",
    "            df=df.values.astype(np.float32)\n",
    "            x_list=[]\n",
    "            y_list=[]\n",
    "            for idx in tqdm(range(0,length)):\n",
    "                #像此处的index的部分，取决于\n",
    "                x=df[idx:idx+seq_len,:-2]\n",
    "                y=df[idx+seq_len-1,-2]\n",
    "                #don't study severe price change\n",
    "                if train:\n",
    "                    if (abs(y)>down_threshold) and (abs(y)<up_threshold):\n",
    "                        x_list.append(x)\n",
    "                        y_list.append(y)\n",
    "                else:\n",
    "                        x_list.append(x)\n",
    "                        y_list.append(y)\n",
    "            length=len(x_list)\n",
    "            return [x_list,y_list,length]\n",
    "        #settle x array and y value in the list so that can get them easily\n",
    "        #final_set=Parallel(cpu_nums)(delayed(judge)(df) for name,df in dataset.groupby('Asset_ID') ) \n",
    "        self.final_set=[judge(df) for name,df in dataset.groupby('Asset_ID')]\n",
    "        self.length=sum([ self.final_set[i][2] for i in range(len( self.final_set))])\n",
    "        self.dataset=dataset\n",
    "        self._x=[ self.final_set[i][0][j] for i in range(len( self.final_set)) for j in range(len( self.final_set[i][0])) ]\n",
    "        self._y=[ self.final_set[i][1][j] for i in range(len( self.final_set)) for j in range(len( self.final_set[i][1])) ]\n",
    "    def __len__(self):\n",
    "        return self.length\n",
    "    def __getitem__(self, idx):\n",
    "        x=self._x[idx]\n",
    "        y=self._y[idx]\n",
    "        return x,y\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████████████| 25385/25385 [00:00<00:00, 179621.75it/s]\n"
     ]
    }
   ],
   "source": [
    "#example\n",
    "test_panel['Asset_ID'] = Asset_ID\n",
    "gd=GruDataset(test_panel,20)\n",
    "# print(\"X: \\n\\n\",gd[100][0],\"Y: \\n\\n\",gd[100][1],\"\\n\\n\")\n",
    "# print(\"length \\n\\n\",len(gd))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#A temo Dataset, used while cut train and validation  \n",
    "class TempDataset(Dataset):\n",
    "        def __init__(self,x,y):\n",
    "            self.x=x\n",
    "            self.y=y\n",
    "        def __len__(self):\n",
    "            return len(self.x)\n",
    "        def __getitem__(self, idx):\n",
    "            return self.x[idx],self.y[idx]\n",
    "\n",
    "def data_split(df,batch_size=512,train_portion=0.75,seq_len=2400):\n",
    "    \"\"\"spilt data in to train set and validation set, and transform \n",
    "    into Grudf in model_structure and DataLoader in pytorch\"\"\"\n",
    "    df_train_x=df[:int(len(df)*train_portion)][0]\n",
    "    df_train_y=df[:int(len(df)*train_portion)][1]\n",
    "    df_valid_x=df[int(len(df)*train_portion):][0]\n",
    "    df_valid_y=df[int(len(df)*train_portion):][1]\n",
    "    df_train=TempDataset(df_train_x,df_train_y)\n",
    "    df_valid=TempDataset(df_valid_x,df_valid_y)\n",
    "    #Dataloader\n",
    "    train_dataloader = DataLoader(df_train, batch_size=batch_size, shuffle=True,drop_last=True)\n",
    "    valid_dataloader = DataLoader(df_valid, batch_size=batch_size, shuffle=True,drop_last=True)\n",
    "    return train_dataloader,valid_dataloader\n",
    "\n",
    "\n",
    "class GRUNN(pl.LightningModule):\n",
    "    def __init__(self,config ,feature_size:int=52):\n",
    "        super().__init__()\n",
    "        self.feature_size=feature_size\n",
    "        self.lr=config['lr']\n",
    "        self.l2=config['l2']\n",
    "        self.dropout=config['dropout']\n",
    "        self.layer_norm = nn.LayerNorm(feature_size)\n",
    "        self.gru = nn.GRU(feature_size,int(feature_size*2/3), 2, batch_first=True, dropout=self.dropout)#hiddenlayer/inputlayer=2/3\n",
    "        self.linear = nn.Linear(int(feature_size*2/3), 1) #dense\n",
    "\n",
    "    def forward(self, x):\n",
    "        #x = self.layer_norm(x)#actually this step is not necessary\n",
    "        out, _ = self.gru(x)#return output,h_n\n",
    "        out = self.linear(out[:, -1])\n",
    "        return out\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        optimizer = torch.optim.Adam(self.parameters(), lr=self.lr,weight_decay=self.l2)\n",
    "        return optimizer\n",
    "\n",
    "    def training_step(self, train_batch, batch_idx):\n",
    "        x, y = train_batch\n",
    "        y_pred = self(x)\n",
    "        # weight = torch.Tensor([1., 0.072, 1.]).cuda()\n",
    "        # loss = F.cross_entropy(y_pred, y, weight=weight)\n",
    "        loss_fun = nn.MSELoss()\n",
    "        loss = loss_fun(y_pred, y)\n",
    "        # loss = F.binary_cross_entropy_with_logits(y_pred, y.view(-1, 1).to(torch.float))\n",
    "        self.log('train/loss', loss.item())\n",
    "        return loss\n",
    "\n",
    "    def predict_step(self, batch, batch_idx: int , dataloader_idx: int = None):\n",
    "        x,y=batch\n",
    "        return [self(x),y]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = {\n",
    "    \"lr\":1e-4,\n",
    "    \"l2\":0.2,\n",
    "    \"dropout\":0.2,\n",
    "}\n",
    "# Transfer to accelerator\n",
    "use_cuda = torch.cuda.is_available()\n",
    "device = torch.device(\"cuda:3\" if use_cuda else \"cpu\")\n",
    "\n",
    "torch.manual_seed(0)\n",
    "torch.cuda.manual_seed(0)\n",
    "np.random.seed(0)\n",
    "\n",
    "model = GRUNN(config\n",
    ").to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|                                                                                             | 0/5 [00:00<?, ?it/s]\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'training_dl' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[15], line 17\u001b[0m\n\u001b[1;32m     14\u001b[0m running_training_loss \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0.0\u001b[39m\n\u001b[1;32m     16\u001b[0m \u001b[38;5;66;03m# Begin training\u001b[39;00m\n\u001b[0;32m---> 17\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m idx, (x_batch, y_batch) \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(training_dl):\n\u001b[1;32m     18\u001b[0m     \u001b[38;5;66;03m# Convert to Tensors\u001b[39;00m\n\u001b[1;32m     19\u001b[0m     x_batch \u001b[38;5;241m=\u001b[39m x_batch\u001b[38;5;241m.\u001b[39mfloat()\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[1;32m     20\u001b[0m     y_batch \u001b[38;5;241m=\u001b[39m y_batch\u001b[38;5;241m.\u001b[39mfloat()\u001b[38;5;241m.\u001b[39mto(device)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'training_dl' is not defined"
     ]
    }
   ],
   "source": [
    "training_losses = []\n",
    "validation_losses = []\n",
    "min_validation_loss = np.Inf\n",
    "validate_every = 2\n",
    "EPOCHS = 5\n",
    "\n",
    "# Set to train mode\n",
    "model.train()\n",
    "\n",
    "for epoch in tqdm(range(EPOCHS)):\n",
    "    # Initialize hidden and cell states with dimension:\n",
    "    # (num_layers * num_directions, batch, hidden_size)\n",
    "#     states = model.init_hidden_states(BATCH_SIZE)\n",
    "    running_training_loss = 0.0\n",
    "\n",
    "    # Begin training\n",
    "    for idx, (x_batch, y_batch) in enumerate(training_dl):\n",
    "        # Convert to Tensors\n",
    "        x_batch = x_batch.float().to(device)\n",
    "        y_batch = y_batch.float().to(device)\n",
    "\n",
    "        # Truncated Backpropagation\n",
    "#         states = [state.detach() for state in states]\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # Make prediction\n",
    "        output, states = model(x_batch, states)\n",
    "\n",
    "        # Calculate loss\n",
    "        loss = criterion(output[:, -1, :], y_batch.squeeze())\n",
    "        loss.backward()\n",
    "        running_training_loss += loss.item()\n",
    "\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), 0.5)\n",
    "        optimizer.step()\n",
    "\n",
    "    # Average loss across timesteps\n",
    "    training_losses.append(running_training_loss / len(training_dl))\n",
    "\n",
    "    if epoch % validate_every == 0:\n",
    "        # Set to eval mode\n",
    "        model.eval()\n",
    "\n",
    "        validation_states = model.init_hidden_states(BATCH_SIZE)\n",
    "        running_validation_loss = 0.0\n",
    "\n",
    "        for idx, (x_batch, y_batch) in enumerate(validation_dl):\n",
    "            # Convert to Tensors\n",
    "            x_batch = x_batch.float().to(device)\n",
    "            y_batch = y_batch.float().to(device)\n",
    "\n",
    "            validation_states = [state.detach() for state in validation_states]\n",
    "            output, validation_states = model(x_batch, validation_states)\n",
    "            validation_loss = criterion(output[:, -1, :], y_batch.squeeze())\n",
    "            running_validation_loss += validation_loss.item()\n",
    "\n",
    "        validation_losses.append(running_validation_loss / len(validation_dl))\n",
    "        # Reset to training mode\n",
    "        model.train()\n",
    "\n",
    "        is_best = running_validation_loss / len(validation_dl) < min_validation_loss\n",
    "\n",
    "        if is_best:\n",
    "            min_validation_loss = running_validation_loss / len(validation_dl)\n",
    "            save_checkpoint(epoch + 1, min_validation_loss, model.state_dict(), optimizer.state_dict())\n",
    "\n",
    "# Visualize loss\n",
    "epoch_count = range(1, len(training_losses) + 1)\n",
    "plt.plot(epoch_count, training_losses, 'r--')\n",
    "plt.legend(['Training Loss'])\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.show()\n",
    "\n",
    "val_epoch_count = range(1, len(validation_losses) + 1)\n",
    "plt.plot(val_epoch_count, validation_losses, 'b--')\n",
    "plt.legend(['Validation loss'])\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
